## A Pytorch Implementation of the Transformer Network
This repository includes pytorch implementations of 《A Submodular Optimization based VAE Transformer Framework for Paraphrase Generation》

## Reference
**Paper**
- Vaswani et al., "Attention is All You Need", NIPS 2017
- Kumar et al., "Submodular optimization-based diverse paraphrasing and its effectiveness in data augmentation", NAACL 2019 

**Code**
- [jadore801120/attention-is-all-you-need](https://github.com/jadore801120/attention-is-all-you-need-pytorch)

-[jayparks/transformer](https://github.com/jayparks/transformer)

- [OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)

-[malllabiisc/DiPS](https://github.com/malllabiisc/DiPS)
